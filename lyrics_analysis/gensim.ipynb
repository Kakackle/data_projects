{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c49fa9a-e43f-418b-bd50-7fccf550ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34710e68-4112-4741-bfdf-5cf604e8bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = 'dataframes/lyrics_combined.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c74529-5941-4373-a563-da132df2603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64a7a0b-d7a9-43c4-a275-78a8327cefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c73899a8-f5f2-4364-8e47-da9645bf93af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>all_lyrics</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaliyah</td>\n",
       "      <td>dirty south can yall really feel me east coas...</td>\n",
       "      <td>pop</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>ive been drinkin ive been drinkin i get filth...</td>\n",
       "      <td>pop</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Britney Spears</td>\n",
       "      <td>oh baby baby oh baby baby  oh baby baby how w...</td>\n",
       "      <td>pop</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carly Rae Jepsen</td>\n",
       "      <td>i threw a wish in the well dont ask me ill ne...</td>\n",
       "      <td>pop</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charli XCX</td>\n",
       "      <td>i was busy thinkin bout boys boys boys always...</td>\n",
       "      <td>pop</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Artist                                         all_lyrics genre  \\\n",
       "0           Aaliyah   dirty south can yall really feel me east coas...   pop   \n",
       "1           Beyoncé   ive been drinkin ive been drinkin i get filth...   pop   \n",
       "2    Britney Spears   oh baby baby oh baby baby  oh baby baby how w...   pop   \n",
       "3  Carly Rae Jepsen   i threw a wish in the well dont ask me ill ne...   pop   \n",
       "4        Charli XCX   i was busy thinkin bout boys boys boys always...   pop   \n",
       "\n",
       "   gender  \n",
       "0  female  \n",
       "1  female  \n",
       "2  female  \n",
       "3  female  \n",
       "4  female  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef845b0-15c7-450a-9fb0-2c884bff195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac29309-61c0-4d78-8342-ca4f650cbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = ' '.join(df['all_lyrics'].str.lower())\n",
    "# for i in df.index:\n",
    "#     text = df['all_lyrics'][i].str.lower()\n",
    "#     ' '.join("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c4abccc-f0db-44bb-be14-30b9856b9f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583735"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8bfef86-31e1-4596-b6cc-5183e4a82ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1600000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d7f960d-0974-4a81-a55f-d76c6ad849da",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_lyrics = nlp(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f1d7c6-07e4-4fc1-a4a0-b95525d85ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s for s in processed_lyrics.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3460cec4-f8f3-4de2-a104-0be148385a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11868\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc172a41-e379-4ac8-a35d-123f1b9dd204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326329\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_lyrics.text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb5fe9-7f70-45dc-8997-e7f806193d25",
   "metadata": {},
   "source": [
    "#### preprocess data for training the model (lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e33b99c-5f0e-41b5-a092-87354d3f9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = [sent.lemma_.split() for sent in processed_lyrics.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be6ad4-29ae-4e67-a351-c813733991d3",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67121077-518f-4bd3-b392-e9ea53eea603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2038\n"
     ]
    }
   ],
   "source": [
    "similar_words_model = Word2Vec(\n",
    "    sentences=processed_sentences,\n",
    "    min_count=10, # Purning the internal dictionary\n",
    "    vector_size=200, # the number of dimensions (N) gensim maps the word onto\n",
    "    window=2, # Define when two words are together, 2 means, 2 words left and 2 words right\n",
    "    compute_loss=True,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "print(len(similar_words_model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23e0699b-b3eb-487a-9b86-86187beaf6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2385789.75\n"
     ]
    }
   ],
   "source": [
    "# getting the training loss\n",
    "training_loss = similar_words_model.get_latest_training_loss()\n",
    "print(f\"Training Loss: {training_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9bdf941-4aaf-4816-aaef-99cc770a4b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('parttime', 0.8531744480133057)\n",
      "('toro', 0.8056215643882751)\n",
      "('gimme', 0.7868713736534119)\n",
      "('jean', 0.7758487462997437)\n",
      "('rape', 0.7753496170043945)\n",
      "('danja', 0.7654772996902466)\n",
      "('ohohoh', 0.7528627514839172)\n",
      "('merry', 0.7471245527267456)\n",
      "('woahwoah', 0.7464218735694885)\n",
      "('mo—', 0.7442200183868408)\n"
     ]
    }
   ],
   "source": [
    "for w, sim in similar_words_model.wv.most_similar('lover'):\n",
    "    print((w, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05026a9-4e4c-4e23-95f3-b621d8bdfb8a",
   "metadata": {},
   "source": [
    "### Get similar out-of-corpus words from a FastText set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb7d8d74-30a0-481e-afde-a9cc04f8e0f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mprocessed_sentences)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(corpus_iterable\u001b[38;5;241m=\u001b[39mprocessed_sentences, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(processed_sentences), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w, sim \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlove\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m20\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m((w, sim))\n",
      "File \u001b[1;32m~\\Desktop\\programowanie_web_etc\\python_projects\\data_projects\\venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:852\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m topn:\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dists\n\u001b[1;32m--> 852\u001b[0m best \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39margsort(dists, topn\u001b[38;5;241m=\u001b[39m\u001b[43mtopn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_keys\u001b[49m\u001b[43m)\u001b[49m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# ignore (don't return) keys from the input\u001b[39;00m\n\u001b[0;32m    854\u001b[0m result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    855\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_key[sim \u001b[38;5;241m+\u001b[39m clip_start], \u001b[38;5;28mfloat\u001b[39m(dists[sim]))\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sim \u001b[38;5;129;01min\u001b[39;00m best \u001b[38;5;28;01mif\u001b[39;00m (sim \u001b[38;5;241m+\u001b[39m clip_start) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_keys\n\u001b[0;32m    857\u001b[0m ]\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(window=2)\n",
    "model.build_vocab(corpus_iterable=processed_sentences)\n",
    "model.train(corpus_iterable=processed_sentences, total_examples=len(processed_sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f49af25d-f75b-4e51-82c2-e8265f8f808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('glove', 0.9545936584472656)\n",
      "('prove', 0.8232774138450623)\n",
      "('lovely', 0.8214894533157349)\n",
      "('lover', 0.8095089793205261)\n",
      "('loving', 0.7901659607887268)\n",
      "('logic', 0.7800371646881104)\n",
      "('loser', 0.7678811550140381)\n",
      "('shove', 0.7645830512046814)\n",
      "('los', 0.763531506061554)\n",
      "('happiness', 0.7496615648269653)\n",
      "('I’ve', 0.7415767312049866)\n",
      "('curve', 0.7402692437171936)\n",
      "('lo', 0.7395698428153992)\n",
      "('pray', 0.7344256043434143)\n",
      "('serve', 0.7340152263641357)\n",
      "('false', 0.7322720289230347)\n",
      "('lovin', 0.732210636138916)\n",
      "('remove', 0.7278366088867188)\n",
      "('lobby', 0.7260144352912903)\n",
      "('nerve', 0.7228999733924866)\n"
     ]
    }
   ],
   "source": [
    "for w, sim in model.wv.most_similar('love', topn=20):\n",
    "    print((w, sim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_venv",
   "language": "python",
   "name": "data_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
